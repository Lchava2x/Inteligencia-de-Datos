# -*- coding: utf-8 -*-
"""DetecciondeAnomalias.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JhD4lt97mW9sh3Izam17sazh7U62MK5G
"""

from google.colab import drive
drive.mount('/content/drive')
import zipfile
import os

# Ruta de la carpeta en tu Google Drive
folder_path = '/content/drive/My Drive/Deteccion de Anomalias'

# Listar los archivos
for root, dirs, files in os.walk(folder_path):
    for file in files:
        print(os.path.join(root, file))

zip_path = '/content/drive/My Drive/Deteccion de Anomalias/MachineLearningCSV.zip'
extract_path = '/content/CICIDS2017'

with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(extract_path)

print("Archivo extraído en:", extract_path)

import os

# Listar los archivos extraídos
extract_path = '/content/CICIDS2017'

for root, dirs, files in os.walk(extract_path):
    for file in files:
        print(os.path.join(root, file))

import pandas as pd

# Ruta del archivo CSV
csv_file_path = '/content/CICIDS2017/MachineLearningCVE/Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv'

# Cargar el archivo CSV
df = pd.read_csv(csv_file_path)

# Mostrar las primeras filas del DataFrame
print(df.head())

# Ver estadísticas generales y los tipos de datos
print(df.info())
print(df.describe())

# Comprobar valores nulos
print(df.isnull().sum())

# Eliminar filas con valores nulos
df = df.dropna()

# columnas categóricas, conviértelas en variables numéricas
df = pd.get_dummies(df)

print(df.head())  # Ver las primeras filas para asegurar que la transformación fue exitosa

# Seleccionar características (X) y etiquetas (y)
X = df.drop([' Label_BENIGN', ' Label_PortScan'], axis=1)  # Asegúrate de incluir el espacio inicial en los nombres de las columnas
y = df[' Label_PortScan']  # Usar 'Label_PortScan' como etiqueta objetivo

# Verifica la forma de los conjuntos
print("Características (X):", X.shape)
print("Etiquetas (y):", y.shape)

import pandas as pd
from sklearn.model_selection import train_test_split


# Dividir datos en entrenamiento y prueba (80% entrenamiento, 20% prueba)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Mostrar los tamaños de los conjuntos de entrenamiento y prueba
print("Tamaño del conjunto de entrenamiento:", X_train.shape)
print("Tamaño del conjunto de prueba:", X_test.shape)

from sklearn.preprocessing import StandardScaler
import numpy as np

# Reemplazar valores infinitos con NaN
X_train = np.where(np.isinf(X_train), np.nan, X_train)
X_test = np.where(np.isinf(X_test), np.nan, X_test)

# Eliminar filas con valores NaN
X_train = X_train[~np.isnan(X_train).any(axis=1)]
X_test = X_test[~np.isnan(X_test).any(axis=1)]

# Escalar los datos
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

import numpy as np

# Reemplazar np.inf y -np.inf por NaN en X_train y X_test
X_train[np.isinf(X_train)] = np.nan
X_test[np.isinf(X_test)] = np.nan

# Asegúrate de que después de reemplazar los infinitos, los NaN estén presentes en los datos
print("Datos después de reemplazar infinitos por NaN:")
print(X_train)
print(X_test)

from sklearn.impute import SimpleImputer

# imputador para reemplazar NaN con la media de cada columna
imputer = SimpleImputer(strategy='mean')

# Ajustar el imputador
X_train_imputed = imputer.fit_transform(X_train)

# Transformar los datos de prueba
X_test_imputed = imputer.transform(X_test)

# Verificar los datos después de la imputación
print("Datos de entrenamiento después de la imputación:")
print(X_train_imputed)
print("Datos de prueba después de la imputación:")
print(X_test_imputed)

import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report
import matplotlib.pyplot as plt
import seaborn as sns

# Reemplazar infinitos y NaN en X
X_train.replace([np.inf, -np.inf], np.nan, inplace=True)
X_test.replace([np.inf, -np.inf], np.nan, inplace=True)

# Reemplazar NaN por el valor medio de cada columna
X_train.fillna(X_train.mean(), inplace=True)
X_test.fillna(X_test.mean(), inplace=True)

# Asegúrate de que tus datos no tengan valores NaN ni infinitos ahora
print(X_train.isnull().sum())  # Verifica si todavía hay valores nulos en X_train
print(X_test.isnull().sum())   # Verifica si todavía hay valores nulos en X_test

# Escalar los datos
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Dividir los datos en conjunto de entrenamiento y prueba
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Definir y entrenar el modelo
model = LogisticRegression(max_iter=1000)
model.fit(X_train_scaled, y_train)

# Hacer las predicciones
y_pred = model.predict(X_test_scaled)

# Generar el reporte de clasificación
report = classification_report(y_test, y_pred, output_dict=True)

# Convertir el reporte a un DataFrame para facilitar la visualización
report_df = pd.DataFrame(report).transpose()

# Visualizar las métricas con un gráfico de barras
plt.figure(figsize=(10, 6))
report_df[['precision', 'recall', 'f1-score']].plot(kind='bar', ax=plt.gca(), colormap='viridis')
plt.title('Métricas de Desempeño del Modelo')
plt.ylabel('Valor')
plt.xlabel('Clase')
plt.xticks(rotation=0)
plt.show()

# Imprimir el reporte completo
print("Reporte de clasificación:\n", classification_report(y_test, y_pred))

import pandas as pd

# Cargar el conjunto de datos desde un archivo CSV (ajusta la ruta si es necesario)
data = pd.read_csv('/content/CICIDS2017/MachineLearningCVE/Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv')

# Ver las primeras filas para asegurarte de que se haya cargado correctamente
print(data.head())

# Eliminar espacios al inicio y al final de los nombres de las columnas
data.columns = data.columns.str.strip()

# Verificar los nombres de las columnas
print(data.columns)

print(data['Label'].unique())

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

def simular_ataque(data):
    # Filtrar el tráfico normal (BENIGN) y los ataques (PortScan)
    normal_traffic = data[data['Label'] == 'BENIGN']  # Tráfico normal
    attack_traffic = data[data['Label'] == 'PortScan']  # Ataque de tipo PortScan

    # Ver las primeras filas de ambos conjuntos filtrados
    print("Tráfico normal:")
    print(normal_traffic.head())
    print("Tráfico de ataque:")
    print(attack_traffic.head())

    # Visualizar la distribución de la duración del flujo entre tráfico normal y ataques
    plt.figure(figsize=(10, 6))
    sns.histplot(normal_traffic['Flow Duration'], color='blue', kde=True, label='Tráfico Normal', stat='density')
    sns.histplot(attack_traffic['Flow Duration'], color='red', kde=True, label='PortScan', stat='density')
    plt.title('Distribución de Duración del Flujo: Tráfico Normal vs. Ataque')
    plt.xlabel('Duración del Flujo')
    plt.ylabel('Densidad')
    plt.legend()
    plt.show()

# Llamar a la función para simular el ataque
simular_ataque(data)

import zipfile

# Ruta del archivo ZIP
zip_path = '/content/drive/My Drive/Deteccion de Anomalias/MachineLearningCSV.zip'

# Abrir el archivo ZIP y listar su contenido
with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.printdir()

import zipfile
import pandas as pd

# Ruta del archivo ZIP
zip_path = '/content/drive/My Drive/Deteccion de Anomalias/MachineLearningCSV.zip'

# Abrir el archivo ZIP y cargar el archivo CSV deseado
with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    file_name = 'MachineLearningCVE/Wednesday-workingHours.pcap_ISCX.csv'  # Cambiar a tu archivo deseado
    data = pd.read_csv(zip_ref.open(file_name))  # Cargar el archivo CSV desde el ZIP

# Verifica que los datos se hayan cargado correctamente
print(data.head())

import zipfile
import os
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Ruta del archivo ZIP
zip_file_path = '/content/drive/My Drive/Deteccion de Anomalias/MachineLearningCSV.zip'

# Carpeta donde se extraerán los archivos del ZIP
extracted_folder = '/content/drive/My Drive/Deteccion de Anomalias/ExtractedCSV/'

# Extraer el contenido del archivo ZIP
with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:
    zip_ref.extractall(extracted_folder)

# Verificar los archivos extraídos
extracted_files = os.listdir(os.path.join(extracted_folder, 'MachineLearningCVE'))
print("Archivos extraídos:", extracted_files)

# Cargar el primer archivo CSV extraído
csv_file = os.path.join(extracted_folder, 'MachineLearningCVE', extracted_files[0])  # Cambia el índice si es necesario
data = pd.read_csv(csv_file)

# Ver las primeras filas y las columnas del archivo para entender su estructura
print("Primeras filas del archivo CSV:")
print(data.head())

print("\nColumnas del archivo CSV:")
print(data.columns)

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Cargar el archivo CSV (ajustar la ruta si es necesario)
csv_file = '/content/drive/My Drive/Deteccion de Anomalias/ExtractedCSV/MachineLearningCVE/Wednesday-workingHours.pcap_ISCX.csv'
data = pd.read_csv(csv_file)

# Eliminar espacios adicionales en los nombres de las columnas
data.columns = data.columns.str.strip()

# Filtrar el tráfico normal (BENIGN) y los ataques (PortScan)
normal_traffic = data[data['Label'] == 'BENIGN']  # Tráfico normal
attack_traffic = data[data['Label'] == 'PortScan']  # Ataque de tipo PortScan

# Ver las primeras filas de ambos conjuntos filtrados (opcional)
print("Tráfico normal:")
print(normal_traffic.head())
print("Tráfico de ataque:")
print(attack_traffic.head())

# Crear histogramas para comparar "Total Fwd Packets" entre ambos tipos de tráfico
plt.figure(figsize=(10, 6))
sns.histplot(normal_traffic['Total Fwd Packets'], color='blue', label='Tráfico Normal', kde=True, stat='density')
sns.histplot(attack_traffic['Total Fwd Packets'], color='red', label='Tráfico de Ataque', kde=True, stat='density')
plt.title('Distribución de Total Fwd Packets entre Tráfico Normal y de Ataque')
plt.xlabel('Total Fwd Packets')
plt.ylabel('Densidad')
plt.legend()
plt.show()